<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Getting start &mdash; pyTensorRT 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="modelConverter Classes" href="modelConverter_src.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            pyTensorRT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#step-1-convert-model-to-onnx">step 1: convert model to onnx</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-convert-onnx-model-to-tensorrt-engine">step 2: convert onnx model to tensorRT engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-3-inference-tensorrt">step 3: inference TensorRt</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">sources:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modelConverter_src.html">modelConverter Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="EngineRT_src.html">EngineRT</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pyTensorRT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Getting start</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/getting_start.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="getting-start">
<h1>Getting start<a class="headerlink" href="#getting-start" title="Permalink to this heading"></a></h1>
<section id="step-1-convert-model-to-onnx">
<h2>step 1: convert model to onnx<a class="headerlink" href="#step-1-convert-model-to-onnx" title="Permalink to this heading"></a></h2>
<p>In the first step, you have to convert your model to the standard and common <code class="docutils literal notranslate"><span class="pre">.onnx</span></code> format.
This section may be different for TensorFlow and PyTorge modules. In this section, we create a model with tensorflow and convert it to <code class="docutils literal notranslate"><span class="pre">onnx</span></code> format
to convert a tensorflow model into an onnx model, we use <code class="docutils literal notranslate"><span class="pre">modelConverter</span></code> modules and its <code class="docutils literal notranslate"><span class="pre">Converter</span></code> class</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">modelConverter</span> <span class="kn">import</span> <span class="n">Converter</span>
</pre></div>
</div>
<p>now lets build a test model Using keras and tensorflow</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">resnet</span><span class="o">.</span><span class="n">ResNet50</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">pooling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span> <span class="n">base_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">input</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">&#39;model.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>to convert a TensorFlow model into a .onnx model, we should save the model in .pb format.
<code class="docutils literal notranslate"><span class="pre">Converter.pb.kerasmodel_to_pb</span></code> method get two arguments. first one is the model and the second is your desired path to save the model as .pb format</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#save model as pb</span>
<span class="n">Converter</span><span class="o">.</span><span class="n">pb</span><span class="o">.</span><span class="n">kerasmodel_to_pb</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;model/&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>After executing this part of the code, a folder named <code class="docutils literal notranslate"><span class="pre">model</span></code> will be created in the project path, which will hold the files related to the <code class="docutils literal notranslate"><span class="pre">.pb</span></code> format.</p>
<p>As the last step, just run the following code snippet to create an onnx moel.
<code class="docutils literal notranslate"><span class="pre">Converter.onnx.pb_to_onnx</span></code> method get two arguments. first one is the path of <code class="docutils literal notranslate"><span class="pre">.pb</span></code> folder  and the second is your desired path to save the model as <code class="docutils literal notranslate"><span class="pre">.onnx</span></code> format</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#convert .pb model into onnx format</span>
<span class="n">Converter</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">pb_to_onnx</span><span class="p">(</span><span class="s1">&#39;model/&#39;</span><span class="p">,</span> <span class="s1">&#39;model.onnx&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-2-convert-onnx-model-to-tensorrt-engine">
<h2>step 2: convert onnx model to tensorRT engine<a class="headerlink" href="#step-2-convert-onnx-model-to-tensorrt-engine" title="Permalink to this heading"></a></h2>
<p>Regardless of how you converted your model to ONNX format, the steps from this point forward will be the same. After creating the model in <strong>ONNX</strong> format, we can now easily create the <strong>tensor RT engine</strong>
for doing this, we using <code class="docutils literal notranslate"><span class="pre">Converter.tensorRT.onnx_to_tensorrt</span></code> method. this method got these arguments.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">onnx_path</span></code> : path of your .onnx file that you built previously.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">res_path</span></code> : your desired path to save the tensor RT model. This file should be <strong>.plan</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> : batch size of your inputs. you can’t change this after building TensorRT engine</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">precision</span></code> : the precision of the tensorRT model. In this version only <code class="docutils literal notranslate"><span class="pre">fp16</span></code> and <code class="docutils literal notranslate"><span class="pre">fp32</span></code> are supported</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_idx</span></code> : index of the GPU device that you want to build your model on it. default is 0 for the first GPU device. use this argument when you have multi GPU</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_memory_size</span></code> : The maximum amount of space that the model can occupy on the graphics card. . This entry is in MB and must be a power of 2</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">modelConverter</span> <span class="kn">import</span> <span class="n">Converter</span>

<span class="c1">#convert onnx model into tensort rt</span>
<span class="n">Converter</span><span class="o">.</span><span class="n">tensorRT</span><span class="o">.</span><span class="n">onnx_to_tensorrt</span><span class="p">(</span><span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span> <span class="s1">&#39;model.plan&#39;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<p>After executing this part of the code, a file named <code class="docutils literal notranslate"><span class="pre">model.plan</span></code> will be created in the project path, which is tensorRT engine</p>
</section>
<section id="step-3-inference-tensorrt">
<h2>step 3: inference TensorRt<a class="headerlink" href="#step-3-inference-tensorrt" title="Permalink to this heading"></a></h2>
<p>After building the TensorRt engine, now it’s time to use it. for using tensorRT engine, we use <code class="docutils literal notranslate"><span class="pre">engineRT</span></code> class of <code class="docutils literal notranslate"><span class="pre">engineRT</span></code> module. for more details see <a class="reference internal" href="EngineRT_src.html"><span class="doc">EngineRT</span></a></p>
<p>for first step we should load engineRT</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>set <code class="docutils literal notranslate"><span class="pre">cuda_idx</span></code> argument 0 if you are using one GPU device</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">EngineRT</span> <span class="kn">import</span> <span class="n">EngineRT</span>

<span class="c1">#load tensorRT engine</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">EngineRT</span><span class="p">(</span><span class="s2">&quot;model.plan&quot;</span><span class="p">,</span> <span class="n">cuda_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we give an arbitrary input to the model with the <code class="docutils literal notranslate"><span class="pre">inference</span></code> method. This method returns the output of the model for the given input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that both input and output are lists of model’s inputs and outputs. If your model has only one input and one output, the input should be a <code class="docutils literal notranslate"><span class="pre">list</span></code> of length 1 including the batch inputs <code class="docutils literal notranslate"><span class="pre">np.array</span></code> and the output will be a <code class="docutils literal notranslate"><span class="pre">list</span></code> of length 1 including the batch outputs <code class="docutils literal notranslate"><span class="pre">np.array</span></code></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate random input. Its shape must match the input shape of the model</span>
<span class="n">test_inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

<span class="c1">#inference inputs into model</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">test_inputs</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; this model has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="si">}</span><span class="s2"> output&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; output shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">this</span> <span class="n">model</span> <span class="n">has</span> <span class="mi">1</span> <span class="n">output</span>
<span class="o">&gt;&gt;</span> <span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modelConverter_src.html" class="btn btn-neutral float-right" title="modelConverter Classes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, amirHossein Malekzadeh.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>